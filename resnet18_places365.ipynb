{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gxjI1VrvIkX",
        "outputId": "91ab4271-a40e-4deb-94ab-e6b584d7f350"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RESULT ON http://places.csail.mit.edu/demo/6.jpg\n",
            "--TYPE OF ENVIRONMENT: indoor\n",
            "--SCENE CATEGORIES:\n",
            "0.299 -> dorm_room\n",
            "0.139 -> youth_hostel\n",
            "0.107 -> hotel_room\n",
            "0.041 -> beauty_salon\n",
            "0.030 -> television_room\n",
            "--SCENE ATTRIBUTES:\n",
            "enclosed area, no horizon, man-made, wood, indoor lighting, glossy, working, glass, cloth\n",
            "Class activation map is saved as cam.jpg\n"
          ]
        }
      ],
      "source": [
        "# PlacesCNN to predict the scene category, attribute, and class activation map in a single pass\n",
        "# by Bolei Zhou, sep 2, 2017\n",
        "# updated, making it compatible to pytorch 1.x in a hacky way\n",
        "\n",
        "import torch\n",
        "from torch.autograd import Variable as V\n",
        "import torchvision.models as models\n",
        "from torchvision import transforms as trn\n",
        "from torch.nn import functional as F\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        " # hacky way to deal with the Pytorch 1.0 update\n",
        "def recursion_change_bn(module):\n",
        "    if isinstance(module, torch.nn.BatchNorm2d):\n",
        "        module.track_running_stats = 1\n",
        "    else:\n",
        "        for i, (name, module1) in enumerate(module._modules.items()):\n",
        "            module1 = recursion_change_bn(module1)\n",
        "    return module\n",
        "\n",
        "def load_labels():\n",
        "    # prepare all the labels\n",
        "    # scene category relevant\n",
        "    file_name_category = 'categories_places365.txt'\n",
        "    if not os.access(file_name_category, os.W_OK):\n",
        "        synset_url = 'https://raw.githubusercontent.com/csailvision/places365/master/categories_places365.txt'\n",
        "        os.system('wget ' + synset_url)\n",
        "    classes = list()\n",
        "    with open(file_name_category) as class_file:\n",
        "        for line in class_file:\n",
        "            classes.append(line.strip().split(' ')[0][3:])\n",
        "    classes = tuple(classes)\n",
        "\n",
        "    # indoor and outdoor relevant\n",
        "    file_name_IO = 'IO_places365.txt'\n",
        "    if not os.access(file_name_IO, os.W_OK):\n",
        "        synset_url = 'https://raw.githubusercontent.com/csailvision/places365/master/IO_places365.txt'\n",
        "        os.system('wget ' + synset_url)\n",
        "    with open(file_name_IO) as f:\n",
        "        lines = f.readlines()\n",
        "        labels_IO = []\n",
        "        for line in lines:\n",
        "            items = line.rstrip().split()\n",
        "            labels_IO.append(int(items[-1]) -1) # 0 is indoor, 1 is outdoor\n",
        "    labels_IO = np.array(labels_IO)\n",
        "\n",
        "    # scene attribute relevant\n",
        "    file_name_attribute = 'labels_sunattribute.txt'\n",
        "    if not os.access(file_name_attribute, os.W_OK):\n",
        "        synset_url = 'https://raw.githubusercontent.com/csailvision/places365/master/labels_sunattribute.txt'\n",
        "        os.system('wget ' + synset_url)\n",
        "    with open(file_name_attribute) as f:\n",
        "        lines = f.readlines()\n",
        "        labels_attribute = [item.rstrip() for item in lines]\n",
        "    file_name_W = 'W_sceneattribute_wideresnet18.npy'\n",
        "    if not os.access(file_name_W, os.W_OK):\n",
        "        synset_url = 'http://places2.csail.mit.edu/models_places365/W_sceneattribute_wideresnet18.npy'\n",
        "        os.system('wget ' + synset_url)\n",
        "    W_attribute = np.load(file_name_W)\n",
        "\n",
        "    return classes, labels_IO, labels_attribute, W_attribute\n",
        "\n",
        "def hook_feature(module, input, output):\n",
        "    features_blobs.append(np.squeeze(output.data.cpu().numpy()))\n",
        "\n",
        "def returnCAM(feature_conv, weight_softmax, class_idx):\n",
        "    # generate the class activation maps upsample to 256x256\n",
        "    size_upsample = (256, 256)\n",
        "    nc, h, w = feature_conv.shape\n",
        "    output_cam = []\n",
        "    for idx in class_idx:\n",
        "        cam = weight_softmax[class_idx].dot(feature_conv.reshape((nc, h*w)))\n",
        "        cam = cam.reshape(h, w)\n",
        "        cam = cam - np.min(cam)\n",
        "        cam_img = cam / np.max(cam)\n",
        "        cam_img = np.uint8(255 * cam_img)\n",
        "        output_cam.append(cv2.resize(cam_img, size_upsample))\n",
        "    return output_cam\n",
        "\n",
        "def returnTF():\n",
        "# load the image transformer\n",
        "    tf = trn.Compose([\n",
        "        trn.Resize((224,224)),\n",
        "        trn.ToTensor(),\n",
        "        trn.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    return tf\n",
        "\n",
        "\n",
        "def load_model():\n",
        "    # this model has a last conv feature map as 14x14\n",
        "\n",
        "    model_file = 'wideresnet18_places365.pth.tar'\n",
        "    if not os.access(model_file, os.W_OK):\n",
        "        os.system('wget http://places2.csail.mit.edu/models_places365/' + model_file)\n",
        "        os.system('wget https://raw.githubusercontent.com/csailvision/places365/master/wideresnet.py')\n",
        "\n",
        "    import wideresnet\n",
        "    model = wideresnet.resnet18(num_classes=365)\n",
        "    checkpoint = torch.load(model_file, map_location=lambda storage, loc: storage)\n",
        "    state_dict = {str.replace(k,'module.',''): v for k,v in checkpoint['state_dict'].items()}\n",
        "    model.load_state_dict(state_dict)\n",
        "    \n",
        "    # hacky way to deal with the upgraded batchnorm2D and avgpool layers...\n",
        "    for i, (name, module) in enumerate(model._modules.items()):\n",
        "        module = recursion_change_bn(model)\n",
        "    model.avgpool = torch.nn.AvgPool2d(kernel_size=14, stride=1, padding=0)\n",
        "    \n",
        "    model.eval()\n",
        "\n",
        "\n",
        "\n",
        "    # the following is deprecated, everything is migrated to python36\n",
        "\n",
        "    ## if you encounter the UnicodeDecodeError when use python3 to load the model, add the following line will fix it. Thanks to @soravux\n",
        "    #from functools import partial\n",
        "    #import pickle\n",
        "    #pickle.load = partial(pickle.load, encoding=\"latin1\")\n",
        "    #pickle.Unpickler = partial(pickle.Unpickler, encoding=\"latin1\")\n",
        "    #model = torch.load(model_file, map_location=lambda storage, loc: storage, pickle_module=pickle)\n",
        "\n",
        "    model.eval()\n",
        "    # hook the feature extractor\n",
        "    features_names = ['layer4','avgpool'] # this is the last conv layer of the resnet\n",
        "    for name in features_names:\n",
        "        model._modules.get(name).register_forward_hook(hook_feature)\n",
        "    return model\n",
        "\n",
        "\n",
        "# load the labels\n",
        "classes, labels_IO, labels_attribute, W_attribute = load_labels()\n",
        "\n",
        "# load the model\n",
        "features_blobs = []\n",
        "model = load_model()\n",
        "\n",
        "# load the transformer\n",
        "tf = returnTF() # image transformer\n",
        "\n",
        "# get the softmax weight\n",
        "params = list(model.parameters())\n",
        "weight_softmax = params[-2].data.numpy()\n",
        "weight_softmax[weight_softmax<0] = 0\n",
        "\n",
        "# load the test image\n",
        "img_url = 'http://places.csail.mit.edu/demo/6.jpg'\n",
        "os.system('wget %s -q -O test.jpg' % img_url)\n",
        "img = Image.open('/content/1.png').convert('RGB')\n",
        "input_img = V(tf(img).unsqueeze(0))\n",
        "\n",
        "# forward pass\n",
        "logit = model.forward(input_img)\n",
        "h_x = F.softmax(logit, 1).data.squeeze()\n",
        "probs, idx = h_x.sort(0, True)\n",
        "probs = probs.numpy()\n",
        "idx = idx.numpy()\n",
        "\n",
        "print('RESULT ON ' + img_url)\n",
        "\n",
        "# output the IO prediction\n",
        "io_image = np.mean(labels_IO[idx[:10]]) # vote for the indoor or outdoor\n",
        "if io_image < 0.5:\n",
        "    print('--TYPE OF ENVIRONMENT: indoor')\n",
        "else:\n",
        "    print('--TYPE OF ENVIRONMENT: outdoor')\n",
        "\n",
        "# output the prediction of scene category\n",
        "print('--SCENE CATEGORIES:')\n",
        "for i in range(0, 5):\n",
        "    print('{:.3f} -> {}'.format(probs[i], classes[idx[i]]))\n",
        "\n",
        "# output the scene attributes\n",
        "responses_attribute = W_attribute.dot(features_blobs[1])\n",
        "idx_a = np.argsort(responses_attribute)\n",
        "print('--SCENE ATTRIBUTES:')\n",
        "print(', '.join([labels_attribute[idx_a[i]] for i in range(-1,-10,-1)]))\n",
        "\n",
        "\n",
        "# generate class activation mapping\n",
        "print('Class activation map is saved as cam.jpg')\n",
        "CAMs = returnCAM(features_blobs[0], weight_softmax, [idx[0]])\n",
        "\n",
        "# render the CAM and output\n",
        "img_name = \"/content/1.png\"\n",
        "img = Image.open(img_name).convert('RGB')\n",
        "#img = cv2.imread('test.jpg')\n",
        "#height, width, _ = img.shape\n",
        "#heatmap = cv2.applyColorMap(cv2.resize(CAMs[0],(width, height)), cv2.COLORMAP_JET)\n",
        "#result = heatmap * 0.4 + img * 0.5\n",
        "#cv2.imwrite('cam.jpg', result)"
      ]
    }
  ]
}